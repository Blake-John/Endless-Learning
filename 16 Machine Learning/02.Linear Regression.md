We use a line $f (x) = wx + b$ to predicts the number, the function also writes as $f_{w,b} (x)$ , which indicate the function will have parameters $a, b$ .

![](imgs/linear_regression.png)

The function $f_{w, b} (x)$ which can predict the result is called **model** . Let look at an example :

![](imgs/samples.png)

In this graph, we have some **input data $x$** and one line $f_{w,b}$ . In the model : 
- the **input data is represented as** $x^{(i)}$ , meaning the $i-th$ data.
- the **real data $y$** is the result , which is the $i-th$ result, represented as $y^{(i)}$ 
- the output of the function $f_{w, b}$ is called the **prediction** , which is represented as $\hat{y}^{(i)}$ , lining on the line of $f_{w, b}$ 

To find a line predict the result most well, we need to find the best $w, b$ , which cause **$\hat{y}^{(i)}$ is closest to $y^{(i)}$ for all $(x^{(i)}, y^{(i)})$** .

# 02 Cost Function

## 2.1 Definition

The cost function is usually the **Mean Squared Error Cost Function (MSE)** :

$$J (w, b) = \dfrac{1}{2m} \sum_{i = 1}^{m} (\hat{y}^{(i)} - y^{(i)})^2$$

In which, $\hat{y}^{(i)} - y^{(i)}$ is called **error** , $m$ is **the number of training examples** , $J (w, b)$ is the **cost function** .

> the factor $\dfrac{1}{2}$ is used to neat the calculation later

So, we have : 

$$J (w, b) = \dfrac{1}{2m} \sum_{i = 1}^{m} (f_{w, b} (x^{(i)})- y^{(i)})^2$$

To find the best line to predict the result, we need to find the parameters $w, b$ which **cause the cost function to be mimimum** , so our **goal is to ${minimize_{w, b} J (w, b)}$ .**

## 2.2 Simplified

To simplify the model, we assume that $b = 0$ , so the model now becomes $f_w (x) = wx$ , the cost function becomes $J (w) = \dfrac{1}{2m} \sum_{i = 1}^m {(wx^{(i)} - y^{(i)})^2}$ .

Let's see how the parameter $w$ affect the cost function $J(w)$ .

![](imgs/costfunction_of_w.png)

We can know, each value of $w$ will lead to a result of $J (w)$ , so **the way to choose $w$ that causes $J (w)$ to be as small as possible** .

## 2.3 Normal

Since we have considered the simplified model, now, we are going to look at the whole model, in which the $w, b$ are both in the $f_{w, b} (x)$ and $J(w, b)$ .

The model function $f_{w, b} (x)$ is still the function of $x$ , but the cost function $J (w, b)$ now have two variable, which is **Bivariate Function** , the the plot of the function is in 3D, such as : 

![](imgs/3d-j.png)

The goal of linear regression is still to find the $w, b$ cause the $J(w, b)$ to be minimum.

# 03 Gradient Descent

## 3.1 Introduction

**Gradient Descent** is the most important algorithm in Machine Learning, Deep Learning and so on.

If we have some functoin $J (w, b)$ for linear regression or any other function $J(w_1, w_2, \dots , w_n, b)$ , we also want to minimize the $J$ by finding some values of $w, b$ .

Now, let's see a visualization of cost function : 

![](imgs/costfunction_normal.png)

When you are standing on the peak, our goal is to start up where you stands and **get to the bottom one of these valleys *as efficiently as possible*** . Each valley is called **local minimum** .

Such as : 

![](imgs/downhill.png)

## 3.2 Math Expression

The gradient descent algorithm in $w$ is : 

$$w = w - \alpha \dfrac{\partial}{\partial w} J (w, b)$$
 
In which , $=$ is **Assignment** in coding, $w$ on the left is the **previous value of $w$** , and the $w$ on the right is the **new value of $w$** , $\alpha$ is called **Learning Rate** , $\dfrac{\partial}{\partial w} J (w, b)$ is the **derivative of function $J$** .

The expression is saying **assign $w$ after by taking the current value of $w$ and adjusting it to a small amount** .

**$\alpha$ Learning Rate** is usually a small positive number between 0 and 1 and what the learning rate does is that **it basically controls how big of a step you take downhill** .

$\dfrac{\partial}{\partial w} J (w, b)$ tells you **in which direction** you want to take your baby step. And in combination with the learning rate $\alpha$ , it also **determine the of the step you want to tak downhill** .

Similarly, the gradient descent algorithm in $b$ is : 

$$b = b - \alpha \dfrac{\partial}{\partial b} J (w, b)$$

So we will repeat the two expressions until **convergence** , which means the function $J (w, b)$ reaches the local minimum.

But one important thing is **the update of $w$ and $b$** . We need to **simultaneously update $w$ and $b$ rather than update them in two terms** . Look at the picture following : 

![](imgs/simultaneously_update.png)

On the left side, **the derivative of $b$ is including the *old value of* $w$** , but in the right side, **the derivative of $b$ is including the *updated value of* $w$** .

## 3.3 Simplified

Let's also consider the cost function with only a parameter $J (w)$ .

So the gradient descent algorithm on $w$ is : 

$$w = w - \alpha \dfrac{\mathrm{d}}{\mathrm{d}w} J (w)$$

We can look at the graph : 

![](imgs/J_w_graph.png)

The part $\dfrac{\mathrm{d}}{\mathrm{d}w} J (w)$ is **the slope of the red line** . When we consider the point in the graph, we can know that the slope of the red line is **positive** , which means $\dfrac{\mathrm{d}}{\mathrm{d}w} J (w)$ is positive. 
So the expression will lead to $w$ **mines a positive value** , resulting a **smaller value of** $w$ . It is, on the graph, the new point will **move to the left** . You may notice that the direction of the changes of $w$ will lead to **a smaller value cost function** $J (w)$ , which is approaching our goal -> **minimize the cost function** .

Similarly, if the initial point of $w$ is at the left part of the graph, the derivative will be negative and the $w$ will increase, which also **causes the decline of the cost function** $J (w)$ .

## 3.4 Learning Rate

If the learning rate $\alpha$ is too small, it will cause the expression to **take a tiny step to change the value** of $w$ . Although the step is too small, the $w$ will still reach the lowest point another time, as the picture shows

![](imgs/small_alpha.png)

But if the learning rate $\alpha$ is too large, it will cause to a result of **taking a step too large and over the lowest point**, as the graph shows. This situation is called **overshoot** , which will lead to the function never reaching the minimum. Worsely, it my cause the function **fails to converge, even becomes *diverge(发散)*** .

![](imgs/large_alpha.png)

Let's see another cost function : 

![](imgs/local_minimum.png)

If the cost function has may valleys, it is said that the function has may point which is local minimum. The slope of these points is $0$ , so the expression $w = w - \alpha \dfrac{\mathrm{d}}{\mathrm{d}w} J (w)$ will cause the $w$ to not be changed. This is why we say the gradient descent algorithm will find out the value of $w$ causes the $J (w)$ to be **local minimum** .

# 04 Linear Regression Model

Now, we are going to dive into the math model of linear regression.

First, let's consider what the part $\dfrac{\partial}{\partial w} J (w, b)$ is :

$$\dfrac{\partial}{\partial w} J (w, b) 
= \dfrac{\partial}{\partial w} \dfrac{1}{2m} \sum_{i = 1}^m{(f_{w, b} (x^{(i)}) - y^{(i)})^2} 
= \dfrac{\partial}{\partial w} \dfrac{1}{2m} \sum_{i = 1}^m {(wx^{(i)} + b - y^{(i)})^2}$$

which yields : 

$$\dfrac{\partial}{\partial w} J (w, b) 
= \dfrac{1}{2m} \sum_{i = 1}^m {(wx^{(i)} + b - y^{(i)})}2x^{(i)}
= \dfrac{1}{m} \sum_{i = 1}^m {(wx^{(i)} + b - y^{(i)})}x^{(i)} \tag{1}$$

And the part $\dfrac{\partial}{\partial b} J (w, b)$ is : 

$$\dfrac{\partial}{\partial b} J (w, b) 
= \dfrac{\partial}{\partial b} \dfrac{1}{2m} \sum_{i = 1}^m{(f_{w, b} (x^{(i)}) - y^{(i)})^2} 
= \dfrac{\partial}{\partial b} \dfrac{1}{2m} \sum_{i = 1}^m {(wx^{(i)} + b - y^{(i)})^2}$$

which yields : 

$$\dfrac{\partial}{\partial b} J (w, b) 
= \dfrac{1}{2m} \sum_{i = 1}^m {(wx^{(i)} + b - y^{(i)})}2
= \dfrac{1}{m} \sum_{i = 1}^m {(wx^{(i)} + b - y^{(i)})} \tag{2}$$

So we will substitue $(1)$ and $(2)$ with the expression $w = w - \alpha \dfrac{\partial}{\partial w} J (w, b)$ and $b = b - \alpha \dfrac{\partial}{\partial b} J (w, b)$ , repeat the step until the cost function reaches to a local minimum.