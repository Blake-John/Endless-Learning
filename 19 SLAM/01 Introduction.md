# 01 What is SLAM ?

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.3](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=10&selection=25,0,56,2&color=yellow)
> SLAM 是 **Simultaneous Localization and Mapping** ，中文译为 **同时定位与地图构建** ，是指搭载 **特定传感器** 的主体，在 **没有环境先验信息** 的情况下，于运动过程中 **建立环境的模型**，同时 **估计自己的运动** 。
> 

以 **相机为主要传感器** 的称为 **视觉SLAM** 。

# 02 What to do ?

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.3](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=10&selection=73,0,77,5&color=note)
> SLAM 的目的是解决“**定位**”与“**地图构建**”这两个问题

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.3](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=10&selection=90,1,92,20&color=note)
> 当用相机作为传感器时，我们要做的，就是根据一张张连续运动的图像（它们形成一段 **视频**），从中 **推断相机的运动**，以及 **周围环境的情况**

> https://github.com/gaoxiang12/slambook

# 03 Example

假设我们要建造一个机器人，那么我们需要他有 **自主运动** 的能力，因此，我们至少需要为他装上 **相机和轮子** 。当然，我们想要这个小机器人能够根据我们的想法到处乱走，那么，他至少需要知道两件事情：
 
> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.15](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=22&selection=5,0,13,5&color=red)
> 1. 我在什么地方？——**定位**。 
> 2. 周围环境是什么样？——**建图**。

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.15](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=22&selection=14,0,23,0&color=yellow)
> “定位”和“建图”，可以看成感知的“内外之分”。作为一个“内外兼修”的小萝卜学家，一方面要 **明白自身的状态**（即 **位置**），另一方面也要 **了解外在的环境**（即 **地图**）

为了实现这些功能，我们可以通过许多种传感器来实现：

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.15](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=22&selection=81,0,90,18&color=note)
> > 一类传感器是 **携带于机器人本体上的**，例如机器人的轮式编码器、相机、激光等等。另一类是 **安装于环境中的**，例如前面讲的导轨、二维码标志等等
> 
> 在rm中，我们的小车会通过场地中特殊的 **rm标识** 来确定自己所在的位置

![视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.15](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=22&rect=45,205,463,451&color=note)

当谈论到 **视觉SLAM** 时，各种各样的 **相机** 就成为了我们机器人的主要传感器。

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.16](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=23&selection=80,21,88,5&color=yellow)
> 按照相机的工作方式，我们把相机分为 **单目（Monocular）**、**双目（Stereo）** 和 **深度相机（RGB-D）** 三个大类

# 04 Camera

我们将看看各种相机的特点及其使用。

## 4.1 单目相机

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.17](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=24&selection=14,0,24,0&color=yellow)
> 只使用一个摄像头进行 SLAM 的做法称为单目 SLAM（Monocular SLAM）

**特点** ：

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.17](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=24&selection=25,0,25,19&color=note)
> 这种传感器结构特别的简单、成本特别的低

单目相机的数据只有一种——照片，而照片有什么特点？

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.17](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=24&selection=31,0,49,9&color=note)
> 照片，本质上是拍照时的场景（Scene），在相机的成像平面上留下的一个 **投影**。它以 **二维的形式反映了三维的世界**。显然，这个过程 **丢掉了场景的一个维度**：也就是所谓的 **深度（或距离）**。在单目相机中，我们 **无法通过单个图片来计算场景中物体离我们的距离**（远近）——之后我们会看到，这个距离将是 SLAM 中非常关键的信息。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.17](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=24&selection=71,21,76,26&color=red)
> 想要恢复三维结构，我们必须 **移动相机的视角** ，才能估计相机的 **运动** ，同时估计场景中 **物体的远近和大小** ，这些信息我们称之为 **结构** 。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.18](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=25&selection=12,0,21,0&color=red)
> 

> 为了能够知道相机的移动，我们需要知道如何去表示相机的 **位置和姿态**， [02 Translation and Rotation](../20%20Robotic/02%20Translation%20and%20Rotation.md) , [03 Rotation Matrix](../20%20Robotic/03%20Rotation%20Matrix.md) , [04 Ratation Angles](../20%20Robotic/04%20Ratation%20Angles.md) , [05 Transformation Matrix](../20%20Robotic/05%20Transformation%20Matrix.md) ^12abc

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.18](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=25&selection=23,31,33,30&color=note)
> 我们还知道近 **处的物体移动快**，**远处的物体则运动缓慢**。于是，当 **相机移动时**，这些物体在图像上的运动，**形成了视差**。通过视差，我们就能定量地判断哪些物体离得远，哪些物体离的近

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.18](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=25&selection=43,3,61,1&color=red)
> 单目 SLAM 估计的轨迹和地图，将与真实的轨迹、地图，**相差一个因子**，也就是所谓的 **尺度（Scale）**。由于单目 SLAM  **无法仅凭图像确定这个真实尺度**，所以又称为 **尺度不确定性**。

为了解决单张图片无法确定深度，我们开始使用双目和深度相机。

## 4.2 双目相机和深度相机

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.18](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=25&selection=85,0,87,24&color=note)
> 双目相机和深度相机的 **目的**，在于通过某种手段 **测量物体离我们的距离**，克服单目无法知道距离的缺点。如果知道了距离，场景的三维结构就可以通过单个图像恢复出来，也就 **消除了尺度不确定性**。

![视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.18](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=25&rect=55,178,460,333&color=note)

### 4.2.1 双目

**原理** ：

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.18](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=25&selection=88,18,95,12&color=yellow)
> 双目相机由 **两个单目相机组成**，但这两个相机之间的距离（称为 **基线（Baseline）**）是已知的。我们通过这个基线来估计每个像素的空间位置——这和人眼非常相似。

**特点** ：

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.19](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=26&selection=8,0,12,12&color=note)
> 1. 计算机上的双目相机 **需要大量的计算** 才能（不太可靠地）估计每一个像素点的深度，相比于人类真是非常的笨拙。

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.19](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=26&selection=15,5,25,19&color=note)
> 2. 双目或多目相机的缺点是 **配置与标定均较为复杂**，其 **深度量程和精度受双目的基线与分辨率限制**，而且视差的计算 **非常消耗计算资源**，需要使用 GPU 和 FPGA 设备加速后，才能实时输出整张图像的距离信息。

### 4.2.2 深度

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.19](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=26&selection=35,0,54,40&color=yellow)
> 深度相机（又称 **RGB-D** 相机，在本书中主要使用 RGB-D 这个名称）是 2010 年左右开始兴起的一种相机，它最大的特点是可以 **通过红外结构光或 Time-of-Flight（ToF）原理**，像激光传感器那样，通过 **主动向物体发射光并接收返回的光**，**测出物体离相机的距离**。

**特点** ：

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.19](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=26&selection=55,22,56,9&color=note)
> 1. 通过 **物理的测量手段**，所以相比于双目可 **节省大量的计算量**

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.19](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=26&selection=73,5,82,3&color=note)
> 2. 测量 **范围窄**、**噪声大**、**视野小**、**易受日光干扰**、**无法测量透射材质** 等诸多问题，在 SLAM 方面，主要用于 **室内 SLAM**，室外则较难应用

# 05 Framework

对于视觉 SLAM，我们主要分为以下几步：

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.20](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=27&selection=74,0,130,23&color=red)
> 1. **传感器信息读取**。在视觉 SLAM 中主要为 **相机图像信息的读取和预处理**。如果在机器人中，还可能有码盘、惯性传感器等信息的读取和同步。 
> 2. **视觉里程计 (Visual Odometry, VO)**。视觉里程计任务是 **估算相邻图像间相机的运动**， 以及局部地图的样子。VO 又称为前端（Front End）。 
> 3. **后端优化（Optimization）**。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，**得到全局一致的轨迹和地图**。由于接在 VO 之后， 又称为后端（Back End）。 
> 4. **回环检测（Loop Closing）**。回环检测 **判断机器人是否曾经到达过先前的位置**。如果检测到回环，它会把信息提供给后端进行处理。 
> 5. **建图（Mapping）**。它根据估计的轨迹，建立与任务要求对应的地图。

![视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.20](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=27&rect=93,366,414,476&color=red)

> [!PDF|important] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.21](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=28&selection=10,10,17,9&color=important)
> 如果把工作环境限定在静态、刚体，光照变化不明显、没有人为干扰的场景，那么，这个 SLAM 系统是相当成熟的了

## 5.1 视觉里程计

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.21](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=28&selection=33,0,36,3&color=yellow)
> 视觉里程计关心 **相邻图像之间的相机运动**，最简单的情况当然是两张图像之间的运动关系。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.21](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=28&selection=73,6,76,1&color=red)
> 在计算机视觉领域，人类在直觉上看来十分自然的事情，在计算机视觉中却非常的困难。**图像在计算机里只是一个数值矩阵**。这个矩阵里表达着什么东西，计算机毫无概念（这也正是现在 **机器学习要解决的问题**）。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.21](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=28&selection=81,33,84,4&color=red)
> 为了定量地估计相机运动，必须在 **了解相机与空间点的几何关系之后进行** ，[01 Introduction](01%20Introduction.md#^12abc) 

**特点** ：

> [!PDF|note] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.22](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=29&selection=12,0,13,10&color=note)
> 叫它为“里程计”是因为它和实际的里程计一样，**只计算相邻时刻的运动**，而和再往前的过去的信息没有关联。

**缺陷** ：

> [!PDF|important] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.22](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=29&selection=34,0,66,4&color=important)
> 仅通过视觉里程计来估计轨迹，将不可避免地出现 **累计漂移（Accumulating Drift）**。这是由于视觉里程计（在最简单的情况下）**只估计两个图像间运动造成的**。
> 
> 我们知道，每次估计都带有一定的误差，而由于里程计的工作方式，先前时刻的误差将会传递到下一时刻， 导致经过一段时间之后，估计的轨迹将不再准确。比方说，机器人先向左转 90 度，再向右转了 90 度。由于误差，我们把第一个 90 度估计成了 89 度。那我们就会尴尬地发现，向右转之后机器人的估计位置并没有回到原点。更糟糕的是，即使之后的估计再准确，与真实值相比，都会带上这-1 度的误差

带有误差的估计如下图所示：
![视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.22](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=29&rect=97,202,399,370&color=important)

> [!PDF|important] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.22](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=29&selection=80,0,86,16&color=important)
> 这也就是所谓的 **漂移（Drift）**。它将导致我们无法建立一致的地图。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.22](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=29&selection=93,0,99,38&color=red)
> 为了解决漂移问题，我们还需要两种技术：**后端优化** 和 **回环检测**。
> 
> 回环检测负责把“机器人回到原始位置”的事情检测出来，而后端优化则根据该信息，校正整个轨迹的形状。

## 5.2 后端优化

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.23](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=30&selection=14,0,21,3&color=yellow)
> 笼统地说，后端优化主要指处理 SLAM 过程中 **噪声的问题**

不论是什么传感器，都会存在噪声，而我们需要在这噪声中找到我们所需要的数据。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.23](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=30&selection=25,29,33,15&color=red)
> 后端优化要考虑的问题， 就是如何 **从这些带有噪声的数据中**，**估计整个系统的状态**，以及这个状态估计的 **不确定性有多大**——这称为 **最大后验概率估计（Maximum-a-Posteriori，MAP）**。这里的状态既包括机器人自身的轨迹，也包含地图。

通常，视觉里程计被称为前端，前端向后端提供数据，后端对数据进行优化。

> [!PDF|important] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.23](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=30&selection=41,21,48,1&color=important)
> 在视觉 SLAM 中，前端和计算机视觉研究领域更为相关，比如图像的特征提取与匹配等，后端则 **主要是滤波与非线性优化算法**。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.23](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=30&selection=77,6,86,0&color=red)
> 我们需要进行**对运动主体自身和周围环境空间不确定性的估计**。为了解决 SLAM，我们需要状态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，去估计状态的均值和不确定性（方差）

## 5.3 回环检测

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.23](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=30&selection=93,0,99,4&color=yellow)
> 回环检测，又称闭环检测（Loop Closure Detection），主要 **解决位置估计随时间漂移的问题**。

回环检测实现在机器人回到原点之后，将实际位置与估计位置之间的漂移消除。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.23](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=30&selection=103,33,109,1&color=red)
> 地图存在的主要意义，是为了让机器人知晓自己到达过的地方。为了实现回环检测，我们需要让机器人具有识别曾到达过的场景的能力。它的实现手段有很多。例如像前面说的那样，我们可以在机器人下方设置一个标志物（如一张二维码图片）。

然而，我们更希望能够直接利用图像来实现这一功能，而不需要外部的帮助，例如，我们希望能够通过 **图像之间的相似性来完成回环检测** 。

## 5.4 建图

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.24](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=31&selection=36,0,43,6&color=yellow)
> 建图（Mapping）是指构建地图的过程。地图是对环境的描述，但这个描述并不是固定的，需要视 SLAM 的应用而定。

![视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.24](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=31&rect=95,155,406,388&color=red)

不同的地图运用于不同的机器人及应用场景。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.24](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=31&selection=71,0,73,26&color=red)
> 对于家用扫地机器人来说，这种主要在低矮平面里运动的机器人，只需要一个二维的地图，标记哪里可以通过，哪里存在障碍物，就够它在一定范围内导航了。而对于一个相机，它有六自由度的运动，我们至少需要一个三维的地图。

各种地图有自己的特点，而大体上，可以将其分为 **度量地图** 和 **拓扑地图** 。

### 5.4.1 度量地图

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.25](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=32&selection=53,0,59,8&color=yellow)
> 度量地图强调精确地表示 **地图中物体的位置关系**，通常我们用 **稀疏（Sparse）** 与 **稠密 （Dense）** 对它们进行分类。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.25](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=32&selection=60,0,67,26&color=red)
> 1. 稀疏地图进行了 **一定程度的抽象**，并 **不需要表达所有的物体**。例如，我们选择一部分具有代表意义的东西，称之为 **路标（Landmark）**，那么一张稀疏地图就是由路标组成的地图，而不是路标的部分就可以忽略掉。

> [!PDF|red] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.25](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=32&selection=67,26,71,0&color=red)
> 相对的，稠密地图着 **重于建模所有看到的东西**。对于定位来说，稀疏路标地图就足够了。而用于导航时，我们往往需要稠密的地图（否则撞上两个路标之间的墙怎么办？）

### 5.4.2 拓扑地图

> [!PDF|yellow] [视觉SLAM十四讲 从理论到实践_高翔; 张涛; 等, p.25](19%20SLAM/视觉SLAM十四讲%20从理论到实践_高翔;%20张涛;%20等.pdf#page=32&selection=94,1,114,13&color=yellow)
> 比于度量地图的精确性，拓扑地图则更 **强调地图元素之间的关系**。拓扑地图是一个图（Graph），**由节点和边组成**，**只考虑节点间的连通性**，例如 A，B 点是连通的，而不考虑如何从 A 点到达 B 点的过程。它放松了地图对精确位置的需要，去掉地图的细节问题， 是一种更为紧凑的表达方式。

